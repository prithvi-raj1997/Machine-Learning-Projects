{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas and numpy for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# matplotlib and seaborn for visuilization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Display all the columns of the dataframe\n",
    "pd.pandas.set_option('display.max_columns',None)\n",
    "\n",
    "# No warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train data after feature engineering: (5822, 103)\n"
     ]
    }
   ],
   "source": [
    "# read in data into dataframe\n",
    "file_train=r'D:\\Machine Learning\\Python Projects\\Project-2\\project\\train.csv'\n",
    "\n",
    "car_train=pd.read_csv(file_train)\n",
    "print('shape of train data after feature engineering: {}'.format(car_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting into train,validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train data: (4948, 103)\n",
      "shape of test data: (874, 103)\n",
      "shape of train1 data: (3958, 103)\n",
      "shape of validation data: (990, 103)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train,test=train_test_split(car_train,test_size=0.15,random_state=123)\n",
    "\n",
    "print('shape of train data: {}'.format(train.shape))\n",
    "print('shape of test data: {}'.format(test.shape))\n",
    "\n",
    "train1, validation=train_test_split(train,test_size=0.2,random_state=456)\n",
    "print('shape of train1 data: {}'.format(train1.shape))\n",
    "print('shape of validation data: {}'.format(validation.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train data: (3958, 102)\n",
      "shape of y_train data: (3958,)\n",
      "shape of x_validation data: (990, 102)\n",
      "shape of y_validation data: (990,)\n",
      "shape of x_test data: (874, 102)\n",
      "shape of y_test data: (874,)\n"
     ]
    }
   ],
   "source": [
    "# Separting independent and dependent variables\n",
    "x_train=train1.drop('V86',axis=1)\n",
    "y_train=train1['V86']\n",
    "\n",
    "print('shape of x_train data: {}'.format(x_train.shape))\n",
    "print('shape of y_train data: {}'.format(y_train.shape))\n",
    "\n",
    "x_validation=validation.drop('V86',axis=1)\n",
    "y_validation=validation['V86']\n",
    "\n",
    "print('shape of x_validation data: {}'.format(x_validation.shape))\n",
    "print('shape of y_validation data: {}'.format(y_validation.shape))\n",
    "\n",
    "x_test=test.drop('V86',axis=1)\n",
    "y_test=test['V86']\n",
    "\n",
    "print('shape of x_test data: {}'.format(x_test.shape))\n",
    "print('shape of y_test data: {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "logr=LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1: Hyper parameter tuning using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'penalty':['l1','l2','elasticnet'],\n",
    "       'C':[2,4,6,8,10],\n",
    "        'fit_intercept':[True,False],\n",
    "       'class_weight':['balanced','None']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:    7.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={'C': [2, 4, 6, 8, 10],\n",
       "                         'class_weight': ['balanced', 'None'],\n",
       "                         'fit_intercept': [True, False],\n",
       "                         'penalty': ['l1', 'l2', 'elasticnet']},\n",
       "             scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logr_grid=GridSearchCV(logr,param_grid=params,scoring='roc_auc',cv=10,verbose=1,n_jobs=-1)\n",
    "logr_grid.fit(x_validation,y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(results,n_top=3):\n",
    "    for i in range(1,n_top+1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score']==i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean Validation Score: {0:.8f} (std:{1:.3f})\".format(\n",
    "                results['mean_test_score'][candidate],\n",
    "                results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean Validation Score: 0.71414932 (std:0.107)\n",
      "Parameters: {'C': 2, 'class_weight': 'None', 'fit_intercept': True, 'penalty': 'l2'}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean Validation Score: 0.71165180 (std:0.104)\n",
      "Parameters: {'C': 2, 'class_weight': 'None', 'fit_intercept': False, 'penalty': 'l2'}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean Validation Score: 0.70798978 (std:0.104)\n",
      "Parameters: {'C': 4, 'class_weight': 'None', 'fit_intercept': True, 'penalty': 'l2'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report(logr_grid.cv_results_,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=2, class_weight='None')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logr_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2: Fitting on the train data with best parameters and predicing on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score,fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.5072475824061558\n",
      "FB2: 0.02092050209205021\n"
     ]
    }
   ],
   "source": [
    "logr_model=LogisticRegression(C=2,penalty='l2',class_weight=None,fit_intercept=True)\n",
    "logr_model.fit(x_train,y_train)\n",
    "\n",
    "# At Ramdon cutoff ---> 0.5\n",
    "predict_test=logr_model.predict(x_test)\n",
    "print('roc_auc_score: {}'.format(roc_auc_score(y_test,predict_test)))\n",
    "print('FB2: {}'.format(fbeta_score(y_test,predict_test,beta=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "train_score=logr_model.predict_proba(x_train)[:,1]\n",
    "real=y_train\n",
    "print(logr_model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs=np.linspace(0,1,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta=2\n",
    "FB_all=[]\n",
    "for cutoff in cutoffs:\n",
    "    predicted=(train_score>cutoff).astype(int)\n",
    "    \n",
    "    TP=((predicted==1) & (real==1)).sum()\n",
    "    TN=((predicted==0) & (real==0)).sum()\n",
    "    FP=((predicted==1) & (real==0)).sum()\n",
    "    FN=((predicted==0) & (real==1)).sum()\n",
    "    \n",
    "    P=TP+FN\n",
    "    N=TN+FP\n",
    "    \n",
    "    precision=TP/(TP+FP)\n",
    "    recall=TP/P\n",
    "    FB=(1+beta**2)*precision*recall/((beta**2)*precision*recall)\n",
    "    FB_all.append(FB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010050251256281407"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mycutoff=cutoffs[FB_all==max(FB_all)][0]\n",
    "mycutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.5760736196319018\n",
      "FB2: 0.2991886409736308\n"
     ]
    }
   ],
   "source": [
    "test_score=logr_model.predict_proba(x_test)[:,1]\n",
    "test_classes=(test_score>mycutoff).astype(int)\n",
    "\n",
    "print('roc_auc_score: {}'.format(roc_auc_score(y_test,test_classes)))\n",
    "print('FB2: {}'.format(fbeta_score(y_test,test_classes,beta=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the Random Forest on Training set\n",
    "from time import time\n",
    "from operator import itemgetter\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "clf=RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1: Hyper parameter tuning using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dist={'n_estimators':[100,150,200,300,500,700,1000],\n",
    "            'criterion':['gini','entropy'],\n",
    "            'max_depth':[3,5,8,None],\n",
    "            'min_samples_split':sp_randint(3,11),\n",
    "            'min_samples_leaf':sp_randint(3,11),\n",
    "            'max_features':sp_randint(3,11),\n",
    "            'bootstrap':[True,False],\n",
    "            'class_weight':['balanced']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   28.1s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2000 out of 2000 | elapsed:  6.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=RandomForestClassifier(), n_iter=200,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'class_weight': ['balanced'],\n",
       "                                        'criterion': ['gini', 'entropy'],\n",
       "                                        'max_depth': [3, 5, 8, None],\n",
       "                                        'max_features': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000019DCD505C70>,\n",
       "                                        'min_samples_leaf': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000019DCD505A30>,\n",
       "                                        'min_samples_split': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000019DCD505520>,\n",
       "                                        'n_estimators': [100, 150, 200, 300,\n",
       "                                                         500, 700, 1000]},\n",
       "                   random_state=789, scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run Randomized Search\n",
    "n_iter_search=200\n",
    "\n",
    "random_search=RandomizedSearchCV(clf,param_distributions=params_dist,n_iter=n_iter_search,scoring='roc_auc',\n",
    "                                n_jobs=-1,cv=10,verbose=1,random_state=789)\n",
    "random_search.fit(x_validation,y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean Validation Score: 0.71402578 (std:0.085)\n",
      "Parameters: {'bootstrap': False, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 3, 'max_features': 3, 'min_samples_leaf': 10, 'min_samples_split': 6, 'n_estimators': 300}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean Validation Score: 0.71144056 (std:0.094)\n",
      "Parameters: {'bootstrap': False, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 3, 'max_features': 8, 'min_samples_leaf': 4, 'min_samples_split': 3, 'n_estimators': 300}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean Validation Score: 0.71057653 (std:0.096)\n",
      "Parameters: {'bootstrap': False, 'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 3, 'max_features': 3, 'min_samples_leaf': 8, 'min_samples_split': 4, 'n_estimators': 1000}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report(random_search.cv_results_,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight='balanced', max_depth=3,\n",
       "                       max_features=3, min_samples_leaf=10, min_samples_split=6,\n",
       "                       n_estimators=300)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'bootstrap': True, 'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': None, 'max_features': 8, 'min_samples_leaf': 5, 'min_samples_split': 9, 'n_estimators': 200}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2: Fitting on the train data with best parameters and predicing on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.6456899240927524\n",
      "FB2: 0.33849129593810445\n"
     ]
    }
   ],
   "source": [
    "# Fit model on train data and predict\n",
    "rf=RandomForestClassifier(bootstrap=False, class_weight='balanced', max_depth=3,\n",
    "                       max_features=3, min_samples_leaf=10, min_samples_split=6,\n",
    "                       n_estimators=300)\n",
    "rf.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "# At Ramdon cutoff ---> 0.5\n",
    "predict_test=rf.predict(x_test)\n",
    "print('roc_auc_score: {}'.format(roc_auc_score(y_test,predict_test)))\n",
    "print('FB2: {}'.format(fbeta_score(y_test,predict_test,beta=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "train_score_rf=rf.predict_proba(x_train)[:,1]\n",
    "real=y_train\n",
    "print(rf.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs=np.linspace(0,1,200)\n",
    "\n",
    "beta=2\n",
    "FB_all=[]\n",
    "for cutoff in cutoffs:\n",
    "    predicted=(train_score_rf>cutoff).astype(int)\n",
    "    \n",
    "    TP=((predicted==1) & (real==1)).sum()\n",
    "    TN=((predicted==0) & (real==0)).sum()\n",
    "    FP=((predicted==1) & (real==0)).sum()\n",
    "    FN=((predicted==0) & (real==1)).sum()\n",
    "    \n",
    "    P=TP+FN\n",
    "    N=TN+FP\n",
    "    \n",
    "    precision=TP/(TP+FP)\n",
    "    recall=TP/P\n",
    "    FB=(1+beta**2)*precision*recall/((beta**2)*precision*recall)\n",
    "    FB_all.append(FB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2964824120603015"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mycutoff_rf=cutoffs[FB_all==max(FB_all)][0]\n",
    "mycutoff_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.5012269938650307\n",
      "FB2: 0.26624548736462095\n"
     ]
    }
   ],
   "source": [
    "test_score_rf=rf.predict_proba(x_test)[:,1]\n",
    "test_classes_rf=(test_score_rf>mycutoff_rf).astype(int)\n",
    "\n",
    "print('roc_auc_score: {}'.format(roc_auc_score(y_test,test_classes_rf)))\n",
    "print('FB2: {}'.format(fbeta_score(y_test,test_classes_rf,beta=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature V2 (0.070586)\n",
      "2. feature V3 (0.050008)\n",
      "3. feature V4 (0.049203)\n",
      "4. feature V6 (0.045950)\n",
      "5. feature V7 (0.045173)\n",
      "6. feature V8 (0.035429)\n",
      "7. feature V9 (0.034680)\n",
      "8. feature V10 (0.032473)\n",
      "9. feature V11 (0.032380)\n",
      "10. feature V12 (0.030558)\n",
      "11. feature V13 (0.026861)\n",
      "12. feature V14 (0.026407)\n",
      "13. feature V15 (0.025254)\n",
      "14. feature V16 (0.024165)\n",
      "15. feature V17 (0.023786)\n",
      "16. feature V18 (0.022499)\n",
      "17. feature V19 (0.022115)\n",
      "18. feature V20 (0.020729)\n",
      "19. feature V21 (0.018590)\n",
      "20. feature V22 (0.017915)\n",
      "21. feature V23 (0.017832)\n",
      "22. feature V24 (0.015624)\n",
      "23. feature V25 (0.015516)\n",
      "24. feature V26 (0.015251)\n",
      "25. feature V27 (0.014987)\n",
      "26. feature V28 (0.014979)\n",
      "27. feature V29 (0.014483)\n",
      "28. feature V30 (0.013319)\n",
      "29. feature V31 (0.012706)\n",
      "30. feature V32 (0.012406)\n",
      "31. feature V33 (0.010604)\n",
      "32. feature V34 (0.010548)\n",
      "33. feature V35 (0.010142)\n",
      "34. feature V36 (0.010069)\n",
      "35. feature V37 (0.009936)\n",
      "36. feature V38 (0.009840)\n",
      "37. feature V39 (0.007230)\n",
      "38. feature V40 (0.006930)\n",
      "39. feature V41 (0.006807)\n",
      "40. feature V42 (0.006620)\n",
      "41. feature V43 (0.006144)\n",
      "42. feature V44 (0.005880)\n",
      "43. feature V45 (0.005731)\n",
      "44. feature V46 (0.005687)\n",
      "45. feature V47 (0.005278)\n",
      "46. feature V48 (0.004916)\n",
      "47. feature V49 (0.004729)\n",
      "48. feature V50 (0.004082)\n",
      "49. feature V51 (0.003906)\n",
      "50. feature V52 (0.003803)\n",
      "51. feature V53 (0.003638)\n",
      "52. feature V54 (0.003560)\n",
      "53. feature V55 (0.003241)\n",
      "54. feature V56 (0.003118)\n",
      "55. feature V57 (0.002897)\n",
      "56. feature V58 (0.002875)\n",
      "57. feature V59 (0.002510)\n",
      "58. feature V60 (0.002446)\n",
      "59. feature V61 (0.002359)\n",
      "60. feature V62 (0.002348)\n",
      "61. feature V63 (0.002060)\n",
      "62. feature V64 (0.002054)\n",
      "63. feature V65 (0.001926)\n",
      "64. feature V66 (0.001806)\n",
      "65. feature V67 (0.001796)\n",
      "66. feature V68 (0.001753)\n",
      "67. feature V69 (0.001718)\n",
      "68. feature V70 (0.001673)\n",
      "69. feature V71 (0.001465)\n",
      "70. feature V72 (0.001252)\n",
      "71. feature V73 (0.001182)\n",
      "72. feature V74 (0.001181)\n",
      "73. feature V75 (0.001128)\n",
      "74. feature V76 (0.000963)\n",
      "75. feature V77 (0.000854)\n",
      "76. feature V78 (0.000822)\n",
      "77. feature V79 (0.000791)\n",
      "78. feature V80 (0.000716)\n",
      "79. feature V81 (0.000680)\n",
      "80. feature V82 (0.000585)\n",
      "81. feature V83 (0.000543)\n",
      "82. feature V84 (0.000494)\n",
      "83. feature V85 (0.000491)\n",
      "84. feature V1_V86_0.06 (0.000395)\n",
      "85. feature V1_V86_0.07 (0.000307)\n",
      "86. feature V1_V86_0.02 (0.000270)\n",
      "87. feature V1_V86_0.04 (0.000270)\n",
      "88. feature V1_V86_0.1 (0.000270)\n",
      "89. feature V1_V86_0.03 (0.000222)\n",
      "90. feature V1_V86_0.05 (0.000188)\n",
      "91. feature V1_V86_0.15 (0.000172)\n",
      "92. feature V1_V86_0.0 (0.000168)\n",
      "93. feature V1_V86_0.08 (0.000074)\n",
      "94. feature V5_8 (0.000000)\n",
      "95. feature V5_3 (0.000000)\n",
      "96. feature V5_9 (0.000000)\n",
      "97. feature V5_1 (0.000000)\n",
      "98. feature V5_5 (0.000000)\n",
      "99. feature V5_7 (0.000000)\n",
      "100. feature V5_2 (0.000000)\n",
      "101. feature V5_10 (0.000000)\n",
      "102. feature V5_6 (0.000000)\n"
     ]
    }
   ],
   "source": [
    "importances = rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(x_train.shape[1]):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, list(x_train.columns)[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Gradient Boosting on Training set\n",
    "from hyperopt import fmin, tpe,hp, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1: Hyper parameter tuning using Bayesian Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter using Bayesian Hyperopt\n",
    "def acc_model(params):\n",
    "    gbm=GradientBoostingClassifier(**params)\n",
    "    return cross_val_score(gbm,x_validation,y_validation).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space={'learning_rate':hp.choice('learning_rate',[0.001,0.01,0.1,0.2]),\n",
    "             'n_estimators':hp.choice('n_estimators',range(200,500)),\n",
    "            'subsample':hp.choice('subsample',[0.2,0.4,0.6,0.8,1]),\n",
    "            'max_depth':hp.choice('max_depth',range(1,10)),\n",
    "            'max_features':hp.choice('max_features',range(5,15)),\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "best=0\n",
    "def f(params):\n",
    "    global best\n",
    "    acc = acc_model(params)\n",
    "    if acc>best:\n",
    "        best=acc\n",
    "    print('new best:',best,params)\n",
    "    return {'loss': -acc,'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best:                                                                                                              \n",
      "0.8484848484848484                                                                                                     \n",
      "{'learning_rate': 0.2, 'max_depth': 6, 'max_features': 9, 'n_estimators': 240, 'subsample': 0.4}                       \n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'learning_rate': 0.01, 'max_depth': 2, 'max_features': 7, 'n_estimators': 213, 'subsample': 0.8}                      \n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'learning_rate': 0.01, 'max_depth': 6, 'max_features': 14, 'n_estimators': 448, 'subsample': 0.6}                     \n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'max_features': 12, 'n_estimators': 457, 'subsample': 1}                        \n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'learning_rate': 0.001, 'max_depth': 6, 'max_features': 12, 'n_estimators': 278, 'subsample': 0.4}                    \n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'learning_rate': 0.1, 'max_depth': 6, 'max_features': 9, 'n_estimators': 418, 'subsample': 0.6}                       \n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'learning_rate': 0.01, 'max_depth': 5, 'max_features': 14, 'n_estimators': 450, 'subsample': 0.4}                     \n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'learning_rate': 0.01, 'max_depth': 5, 'max_features': 6, 'n_estimators': 481, 'subsample': 0.8}                      \n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'learning_rate': 0.001, 'max_depth': 6, 'max_features': 14, 'n_estimators': 420, 'subsample': 0.6}                    \n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'learning_rate': 0.1, 'max_depth': 3, 'max_features': 11, 'n_estimators': 488, 'subsample': 0.6}                      \n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'learning_rate': 0.2, 'max_depth': 6, 'max_features': 9, 'n_estimators': 359, 'subsample': 0.8}                       \n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'max_features': 12, 'n_estimators': 377, 'subsample': 0.6}                      \n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'learning_rate': 0.2, 'max_depth': 9, 'max_features': 13, 'n_estimators': 300, 'subsample': 0.4}                      \n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'learning_rate': 0.1, 'max_depth': 4, 'max_features': 13, 'n_estimators': 464, 'subsample': 0.4}                      \n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'learning_rate': 0.001, 'max_depth': 1, 'max_features': 10, 'n_estimators': 494, 'subsample': 0.6}                    \n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'learning_rate': 0.1, 'max_depth': 8, 'max_features': 5, 'n_estimators': 469, 'subsample': 0.8}                       \n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'learning_rate': 0.01, 'max_depth': 3, 'max_features': 5, 'n_estimators': 449, 'subsample': 0.8}                      \n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'learning_rate': 0.01, 'max_depth': 9, 'max_features': 7, 'n_estimators': 236, 'subsample': 1}                        \n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'learning_rate': 0.2, 'max_depth': 8, 'max_features': 9, 'n_estimators': 369, 'subsample': 0.8}                       \n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'learning_rate': 0.1, 'max_depth': 8, 'max_features': 5, 'n_estimators': 426, 'subsample': 0.4}                       \n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'learning_rate': 0.001, 'max_depth': 2, 'max_features': 7, 'n_estimators': 371, 'subsample': 0.2}                     \n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'learning_rate': 0.001, 'max_depth': 2, 'max_features': 7, 'n_estimators': 367, 'subsample': 0.2}                     \n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'learning_rate': 0.001, 'max_depth': 1, 'max_features': 10, 'n_estimators': 368, 'subsample': 0.2}                    \n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'learning_rate': 0.001, 'max_depth': 1, 'max_features': 10, 'n_estimators': 494, 'subsample': 0.2}                    \n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'learning_rate': 0.001, 'max_depth': 1, 'max_features': 10, 'n_estimators': 435, 'subsample': 0.2}                    \n",
      "100%|███████████████████████████████████████████████| 25/25 [01:09<00:00,  2.79s/trial, best loss: -0.9434343434343434]\n",
      "best: {'learning_rate': 1, 'max_depth': 1, 'max_features': 2, 'n_estimators': 13, 'subsample': 3}\n"
     ]
    }
   ],
   "source": [
    "trials=Trials()\n",
    "best=fmin(f,param_space,algo=tpe.suggest,max_evals=25,trials=trials)\n",
    "print('best: {}'.format(best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: best: {'learning_rate': 0.01, 'max_depth': 4, 'max_features': 12, 'n_estimators': 226, 'subsample': 0.2}\n",
    "\n",
    "2: best : {'learning_rate': 0.01, 'max_depth': 2, 'max_features': 7, 'n_estimators': 214, 'subsample': 0.8}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2: Fitting on the train data with best parameters and predicing on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.5\n",
      "FB2: 0.0\n"
     ]
    }
   ],
   "source": [
    "gbm_bayesian=GradientBoostingClassifier(**{'learning_rate': 0.01, 'max_depth': 2, 'max_features': 7, \n",
    "                                           'n_estimators': 214, 'subsample': 0.8})\n",
    "gbm_bayesian.fit(x_train,y_train)\n",
    "\n",
    "predict_gbm=gbm_bayesian.predict(x_test)\n",
    "print('roc_auc_score: {}'.format(roc_auc_score(y_test,predict_gbm)))\n",
    "print('FB2: {}'.format(fbeta_score(y_test,predict_gbm,beta=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "train_score_gbm=gbm_bayesian.predict_proba(x_train)[:,1]\n",
    "real=y_train\n",
    "print(gbm_bayesian.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs=np.linspace(0,1,500)\n",
    "\n",
    "beta=2\n",
    "FB_all=[]\n",
    "for cutoff in cutoffs:\n",
    "    predicted=(train_score_gbm>cutoff).astype(int)\n",
    "    \n",
    "    TP=((predicted==1) & (real==1)).sum()\n",
    "    TN=((predicted==0) & (real==0)).sum()\n",
    "    FP=((predicted==1) & (real==0)).sum()\n",
    "    FN=((predicted==0) & (real==1)).sum()\n",
    "    \n",
    "    P=TP+FN\n",
    "    N=TN+FP\n",
    "    \n",
    "    precision=TP/(TP+FP)\n",
    "    recall=TP/P\n",
    "    FB=(1+beta**2)*precision*recall/((beta**2)*precision*recall)\n",
    "    FB_all.append(FB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.044088176352705406"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mycutoff_gbm=cutoffs[FB_all==max(FB_all)][0]\n",
    "mycutoff_gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.6192055734636581\n",
      "FB2: 0.32019704433497537\n"
     ]
    }
   ],
   "source": [
    "test_score_gbm=gbm_bayesian.predict_proba(x_test)[:,1]\n",
    "test_classes_gbm=(test_score_gbm>mycutoff_gbm).astype(int)\n",
    "\n",
    "print('roc_auc_score: {}'.format(roc_auc_score(y_test,test_classes_gbm)))\n",
    "print('FB2: {}'.format(fbeta_score(y_test,test_classes_gbm,beta=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting xgboost on Trining set\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1: Hyper parameter tuning using Bayesian Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning using Bayesian Hyperopt\n",
    "def acc_model(params):\n",
    "    xgb=XGBClassifier(**params)\n",
    "    return cross_val_score(xgb,x_validation,y_validation).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space={'n_estimators':hp.choice('n_estimators',[300,500,600,700,800,1000]),\n",
    "            'max_depth':hp.choice('max_depth',[2,3,4,5,6,7,8]),\n",
    "            'learning_rate':hp.choice('learning_rate',[0.001,0.01,0.1,0.2,0.5]),\n",
    "            'gamma':hp.choice('gamma',[i/10.0 for i in range(0,5)]),\n",
    "            'min_child_weight':hp.choice('min_child_weight',[4,5,6,7]),\n",
    "            'subsample':hp.choice('subsample',[i/10.0 for i in range(3,10)]),\n",
    "            'colsample_bytree':hp.choice('colsample_bytree',[i/10.0 for i in range(3,10)]),\n",
    "            'reg_alpha':hp.choice('reg_alpha',[0.1,5,10,40,50,70]),\n",
    "            'reg_lambda':hp.choice('reg_lambda',[1,5,10,15,50]),\n",
    "            'scale_pos_weight':hp.choice('scale_pos_weight',[2,3,4,5,6,7,8,9])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "best=0\n",
    "def f(params):\n",
    "    global best\n",
    "    acc = acc_model(params)\n",
    "    if acc>best:\n",
    "        best=acc\n",
    "    print('new best:',best,params)\n",
    "    return {'loss':-acc, 'status':STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best:                                                                                                              \n",
      "0.9131313131313131                                                                                                     \n",
      "{'colsample_bytree': 0.4, 'gamma': 0.3, 'learning_rate': 0.1, 'max_depth': 8, 'min_child_weight': 6, 'n_estimators': 1000, 'reg_alpha': 5, 'reg_lambda': 5, 'scale_pos_weight': 9, 'subsample': 0.9}\n",
      "new best:                                                                                                              \n",
      "0.9131313131313131                                                                                                     \n",
      "{'colsample_bytree': 0.4, 'gamma': 0.3, 'learning_rate': 0.1, 'max_depth': 2, 'min_child_weight': 4, 'n_estimators': 800, 'reg_alpha': 10, 'reg_lambda': 5, 'scale_pos_weight': 7, 'subsample': 0.5}\n",
      "new best:                                                                                                              \n",
      "0.9282828282828282                                                                                                     \n",
      "{'colsample_bytree': 0.3, 'gamma': 0.3, 'learning_rate': 0.5, 'max_depth': 2, 'min_child_weight': 6, 'n_estimators': 800, 'reg_alpha': 40, 'reg_lambda': 5, 'scale_pos_weight': 7, 'subsample': 0.3}\n",
      "new best:                                                                                                              \n",
      "0.9282828282828282                                                                                                     \n",
      "{'colsample_bytree': 0.4, 'gamma': 0.1, 'learning_rate': 0.01, 'max_depth': 3, 'min_child_weight': 4, 'n_estimators': 500, 'reg_alpha': 10, 'reg_lambda': 50, 'scale_pos_weight': 9, 'subsample': 0.7}\n",
      "new best:                                                                                                              \n",
      "0.9282828282828282                                                                                                     \n",
      "{'colsample_bytree': 0.6, 'gamma': 0.4, 'learning_rate': 0.5, 'max_depth': 2, 'min_child_weight': 4, 'n_estimators': 500, 'reg_alpha': 0.1, 'reg_lambda': 10, 'scale_pos_weight': 7, 'subsample': 0.5}\n",
      "new best:                                                                                                              \n",
      "0.9282828282828282                                                                                                     \n",
      "{'colsample_bytree': 0.7, 'gamma': 0.4, 'learning_rate': 0.2, 'max_depth': 6, 'min_child_weight': 7, 'n_estimators': 700, 'reg_alpha': 40, 'reg_lambda': 10, 'scale_pos_weight': 7, 'subsample': 0.5}\n",
      "new best:                                                                                                              \n",
      "0.9282828282828282                                                                                                     \n",
      "{'colsample_bytree': 0.3, 'gamma': 0.1, 'learning_rate': 0.5, 'max_depth': 5, 'min_child_weight': 6, 'n_estimators': 1000, 'reg_alpha': 10, 'reg_lambda': 50, 'scale_pos_weight': 4, 'subsample': 0.3}\n",
      "new best:                                                                                                              \n",
      "0.9282828282828282                                                                                                     \n",
      "{'colsample_bytree': 0.7, 'gamma': 0.4, 'learning_rate': 0.2, 'max_depth': 3, 'min_child_weight': 7, 'n_estimators': 800, 'reg_alpha': 5, 'reg_lambda': 1, 'scale_pos_weight': 4, 'subsample': 0.4}\n",
      "new best:                                                                                                              \n",
      "0.9282828282828282                                                                                                     \n",
      "{'colsample_bytree': 0.5, 'gamma': 0.4, 'learning_rate': 0.2, 'max_depth': 5, 'min_child_weight': 4, 'n_estimators': 500, 'reg_alpha': 50, 'reg_lambda': 1, 'scale_pos_weight': 9, 'subsample': 0.6}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.6, 'gamma': 0.2, 'learning_rate': 0.001, 'max_depth': 6, 'min_child_weight': 6, 'n_estimators': 800, 'reg_alpha': 50, 'reg_lambda': 50, 'scale_pos_weight': 5, 'subsample': 0.4}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.7, 'gamma': 0.0, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 4, 'n_estimators': 500, 'reg_alpha': 5, 'reg_lambda': 5, 'scale_pos_weight': 6, 'subsample': 0.8}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.6, 'gamma': 0.0, 'learning_rate': 0.001, 'max_depth': 6, 'min_child_weight': 7, 'n_estimators': 700, 'reg_alpha': 40, 'reg_lambda': 5, 'scale_pos_weight': 5, 'subsample': 0.8}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.4, 'gamma': 0.2, 'learning_rate': 0.2, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 500, 'reg_alpha': 70, 'reg_lambda': 5, 'scale_pos_weight': 4, 'subsample': 0.7}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.7, 'gamma': 0.0, 'learning_rate': 0.5, 'max_depth': 4, 'min_child_weight': 7, 'n_estimators': 1000, 'reg_alpha': 10, 'reg_lambda': 10, 'scale_pos_weight': 6, 'subsample': 0.6}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.5, 'gamma': 0.2, 'learning_rate': 0.01, 'max_depth': 5, 'min_child_weight': 6, 'n_estimators': 300, 'reg_alpha': 50, 'reg_lambda': 1, 'scale_pos_weight': 3, 'subsample': 0.3}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.7, 'gamma': 0.4, 'learning_rate': 0.01, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 600, 'reg_alpha': 40, 'reg_lambda': 10, 'scale_pos_weight': 6, 'subsample': 0.9}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.4, 'gamma': 0.2, 'learning_rate': 0.001, 'max_depth': 3, 'min_child_weight': 7, 'n_estimators': 300, 'reg_alpha': 0.1, 'reg_lambda': 50, 'scale_pos_weight': 3, 'subsample': 0.4}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.3, 'gamma': 0.3, 'learning_rate': 0.2, 'max_depth': 3, 'min_child_weight': 4, 'n_estimators': 500, 'reg_alpha': 70, 'reg_lambda': 15, 'scale_pos_weight': 9, 'subsample': 0.9}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.5, 'gamma': 0.3, 'learning_rate': 0.001, 'max_depth': 8, 'min_child_weight': 4, 'n_estimators': 300, 'reg_alpha': 40, 'reg_lambda': 10, 'scale_pos_weight': 6, 'subsample': 0.7}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.4, 'gamma': 0.3, 'learning_rate': 0.001, 'max_depth': 6, 'min_child_weight': 7, 'n_estimators': 800, 'reg_alpha': 10, 'reg_lambda': 1, 'scale_pos_weight': 3, 'subsample': 0.5}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.8, 'gamma': 0.2, 'learning_rate': 0.001, 'max_depth': 6, 'min_child_weight': 6, 'n_estimators': 300, 'reg_alpha': 0.1, 'reg_lambda': 50, 'scale_pos_weight': 8, 'subsample': 0.4}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.9, 'gamma': 0.2, 'learning_rate': 0.001, 'max_depth': 8, 'min_child_weight': 5, 'n_estimators': 300, 'reg_alpha': 0.1, 'reg_lambda': 15, 'scale_pos_weight': 3, 'subsample': 0.7}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.5, 'gamma': 0.3, 'learning_rate': 0.001, 'max_depth': 8, 'min_child_weight': 7, 'n_estimators': 600, 'reg_alpha': 10, 'reg_lambda': 1, 'scale_pos_weight': 8, 'subsample': 0.7}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.6, 'gamma': 0.0, 'learning_rate': 0.001, 'max_depth': 6, 'min_child_weight': 7, 'n_estimators': 700, 'reg_alpha': 40, 'reg_lambda': 1, 'scale_pos_weight': 2, 'subsample': 0.8}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.6, 'gamma': 0.0, 'learning_rate': 0.001, 'max_depth': 6, 'min_child_weight': 7, 'n_estimators': 700, 'reg_alpha': 10, 'reg_lambda': 1, 'scale_pos_weight': 5, 'subsample': 0.8}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.4, 'gamma': 0.2, 'learning_rate': 0.2, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 800, 'reg_alpha': 70, 'reg_lambda': 5, 'scale_pos_weight': 4, 'subsample': 0.5}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.9, 'gamma': 0.0, 'learning_rate': 0.2, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 700, 'reg_alpha': 70, 'reg_lambda': 15, 'scale_pos_weight': 2, 'subsample': 0.8}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.9, 'gamma': 0.0, 'learning_rate': 0.01, 'max_depth': 5, 'min_child_weight': 5, 'n_estimators': 300, 'reg_alpha': 50, 'reg_lambda': 15, 'scale_pos_weight': 2, 'subsample': 0.3}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.9, 'gamma': 0.1, 'learning_rate': 0.01, 'max_depth': 5, 'min_child_weight': 5, 'n_estimators': 300, 'reg_alpha': 0.1, 'reg_lambda': 15, 'scale_pos_weight': 2, 'subsample': 0.3}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.9, 'gamma': 0.0, 'learning_rate': 0.01, 'max_depth': 4, 'min_child_weight': 6, 'n_estimators': 700, 'reg_alpha': 70, 'reg_lambda': 15, 'scale_pos_weight': 2, 'subsample': 0.8}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.8, 'gamma': 0.2, 'learning_rate': 0.01, 'max_depth': 5, 'min_child_weight': 6, 'n_estimators': 300, 'reg_alpha': 50, 'reg_lambda': 15, 'scale_pos_weight': 3, 'subsample': 0.3}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.8, 'gamma': 0.1, 'learning_rate': 0.01, 'max_depth': 5, 'min_child_weight': 6, 'n_estimators': 300, 'reg_alpha': 0.1, 'reg_lambda': 15, 'scale_pos_weight': 3, 'subsample': 0.3}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.8, 'gamma': 0.1, 'learning_rate': 0.01, 'max_depth': 4, 'min_child_weight': 6, 'n_estimators': 600, 'reg_alpha': 70, 'reg_lambda': 15, 'scale_pos_weight': 2, 'subsample': 0.8}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.8, 'gamma': 0.2, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 6, 'n_estimators': 300, 'reg_alpha': 50, 'reg_lambda': 50, 'scale_pos_weight': 3, 'subsample': 0.4}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.4, 'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 2, 'min_child_weight': 7, 'n_estimators': 300, 'reg_alpha': 0.1, 'reg_lambda': 50, 'scale_pos_weight': 3, 'subsample': 0.4}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.6, 'gamma': 0.2, 'learning_rate': 0.001, 'max_depth': 3, 'min_child_weight': 6, 'n_estimators': 800, 'reg_alpha': 50, 'reg_lambda': 50, 'scale_pos_weight': 5, 'subsample': 0.4}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.6, 'gamma': 0.3, 'learning_rate': 0.001, 'max_depth': 2, 'min_child_weight': 7, 'n_estimators': 800, 'reg_alpha': 10, 'reg_lambda': 1, 'scale_pos_weight': 5, 'subsample': 0.5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.5, 'gamma': 0.3, 'learning_rate': 0.5, 'max_depth': 8, 'min_child_weight': 4, 'n_estimators': 1000, 'reg_alpha': 40, 'reg_lambda': 10, 'scale_pos_weight': 6, 'subsample': 0.7}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.5, 'gamma': 0.3, 'learning_rate': 0.001, 'max_depth': 8, 'min_child_weight': 4, 'n_estimators': 700, 'reg_alpha': 40, 'reg_lambda': 10, 'scale_pos_weight': 7, 'subsample': 0.7}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.4, 'gamma': 0.3, 'learning_rate': 0.001, 'max_depth': 3, 'min_child_weight': 6, 'n_estimators': 800, 'reg_alpha': 5, 'reg_lambda': 50, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.3, 'gamma': 0.4, 'learning_rate': 0.1, 'max_depth': 6, 'min_child_weight': 7, 'n_estimators': 800, 'reg_alpha': 10, 'reg_lambda': 1, 'scale_pos_weight': 5, 'subsample': 0.5}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.8, 'gamma': 0.3, 'learning_rate': 0.5, 'max_depth': 5, 'min_child_weight': 6, 'n_estimators': 800, 'reg_alpha': 50, 'reg_lambda': 1, 'scale_pos_weight': 3, 'subsample': 0.6}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.8, 'gamma': 0.3, 'learning_rate': 0.5, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 800, 'reg_alpha': 70, 'reg_lambda': 5, 'scale_pos_weight': 4, 'subsample': 0.6}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.4, 'gamma': 0.4, 'learning_rate': 0.5, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 800, 'reg_alpha': 70, 'reg_lambda': 5, 'scale_pos_weight': 4, 'subsample': 0.6}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.8, 'gamma': 0.4, 'learning_rate': 0.5, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 800, 'reg_alpha': 70, 'reg_lambda': 5, 'scale_pos_weight': 4, 'subsample': 0.6}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.8, 'gamma': 0.4, 'learning_rate': 0.5, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 1000, 'reg_alpha': 5, 'reg_lambda': 5, 'scale_pos_weight': 9, 'subsample': 0.6}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.9, 'gamma': 0.0, 'learning_rate': 0.01, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 600, 'reg_alpha': 50, 'reg_lambda': 50, 'scale_pos_weight': 7, 'subsample': 0.3}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.6, 'gamma': 0.0, 'learning_rate': 0.2, 'max_depth': 6, 'min_child_weight': 7, 'n_estimators': 700, 'reg_alpha': 40, 'reg_lambda': 1, 'scale_pos_weight': 2, 'subsample': 0.8}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.6, 'gamma': 0.2, 'learning_rate': 0.2, 'max_depth': 2, 'min_child_weight': 7, 'n_estimators': 700, 'reg_alpha': 40, 'reg_lambda': 5, 'scale_pos_weight': 4, 'subsample': 0.9}\n",
      "new best:                                                                                                              \n",
      "0.9434343434343434                                                                                                     \n",
      "{'colsample_bytree': 0.9, 'gamma': 0.0, 'learning_rate': 0.2, 'max_depth': 2, 'min_child_weight': 5, 'n_estimators': 500, 'reg_alpha': 40, 'reg_lambda': 5, 'scale_pos_weight': 4, 'subsample': 0.9}\n",
      "100%|███████████████████████████████████████████████| 50/50 [01:22<00:00,  1.65s/trial, best loss: -0.9434343434343434]\n",
      "best: {'colsample_bytree': 3, 'gamma': 2, 'learning_rate': 0, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 4, 'reg_alpha': 4, 'reg_lambda': 4, 'scale_pos_weight': 3, 'subsample': 1}\n"
     ]
    }
   ],
   "source": [
    "trials=Trials()\n",
    "best=fmin(f,param_space,algo=tpe.suggest,max_evals=50,trials=trials)\n",
    "print('best: {}'.format(best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'colsample_bytree': 0.4, 'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 5, 'n_estimators': 800, 'reg_alpha': 40, 'reg_lambda': 15, 'scale_pos_weight': 5, 'subsample': 0.5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'colsample_bytree': 0.6, 'gamma': 0.2, 'learning_rate': 0.001, 'max_depth': 6, 'min_child_weight': 6, 'n_estimators': 800, 'reg_alpha': 50, 'reg_lambda': 50, 'scale_pos_weight': 5, 'subsample': 0.4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2: Fitting the on train data with best parameters and predicing on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.5\n",
      "FB2: 0.0\n"
     ]
    }
   ],
   "source": [
    "xgb_bayesian=XGBClassifier(**{'colsample_bytree': 0.6, 'gamma': 0.2, 'learning_rate': 0.001, \n",
    "                              'max_depth': 6, 'min_child_weight': 6, 'n_estimators': 800, \n",
    "                              'reg_alpha': 50, 'reg_lambda': 50, 'scale_pos_weight': 5, \n",
    "                              'subsample': 0.4})\n",
    "xgb_bayesian.fit(x_train,y_train)\n",
    "\n",
    "predict_xgb=xgb_bayesian.predict(x_test)\n",
    "print('roc_auc_score: {}'.format(roc_auc_score(y_test,predict_xgb)))\n",
    "print('FB2: {}'.format(fbeta_score(y_test,predict_xgb,beta=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "train_score_xgb=xgb_bayesian.predict_proba(x_train)[:,1]\n",
    "real=y_train\n",
    "print(xgb_bayesian.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs=np.linspace(0,1,500)\n",
    "\n",
    "beta=2\n",
    "FB_all=[]\n",
    "for cutoff in cutoffs:\n",
    "    predicted=(train_score_xgb>cutoff).astype(int)\n",
    "    \n",
    "    TP=((predicted==1) & (real==1)).sum()\n",
    "    TN=((predicted==0) & (real==0)).sum()\n",
    "    FP=((predicted==1) & (real==0)).sum()\n",
    "    FN=((predicted==0) & (real==1)).sum()\n",
    "    \n",
    "    P=TP+FN\n",
    "    N=TN+FP\n",
    "    \n",
    "    precision=TP/(TP+FP)\n",
    "    recall=TP/P\n",
    "    FB=(1+beta**2)*precision*recall/((beta**2)*precision*recall)\n",
    "    FB_all.append(FB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3567134268537074"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mycutoff_xgb=cutoffs[FB_all==max(FB_all)][0]\n",
    "mycutoff_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.6414058438182386\n",
      "FB2: 0.33508541392904073\n"
     ]
    }
   ],
   "source": [
    "test_score_xgb=xgb_bayesian.predict_proba(x_test)[:,1]\n",
    "test_classes_xgb=(test_score_xgb>mycutoff_xgb).astype(int)\n",
    "\n",
    "print('roc_auc_score: {}'.format(roc_auc_score(y_test,test_classes_xgb)))\n",
    "print('FB2: {}'.format(fbeta_score(y_test,test_classes_xgb,beta=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn=KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1: Hyper parameter tuning using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'n_neighbors':[10,20,30,40,50],\n",
    "       'weights':['uniform','distance'],\n",
    "       'p':[1,2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    4.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=KNeighborsClassifier(), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'n_neighbors': [10, 20, 30, 40, 50],\n",
       "                                        'p': [1, 2],\n",
       "                                        'weights': ['uniform', 'distance']},\n",
       "                   scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_random=RandomizedSearchCV(knn,param_distributions=params,cv=10,scoring='roc_auc',\n",
    "                             n_iter=100,verbose=1,n_jobs=-1)\n",
    "knn_random.fit(x_validation,y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean Validation Score: 0.64086593 (std:0.102)\n",
      "Parameters: {'weights': 'uniform', 'p': 2, 'n_neighbors': 50}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean Validation Score: 0.62710288 (std:0.135)\n",
      "Parameters: {'weights': 'distance', 'p': 2, 'n_neighbors': 30}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean Validation Score: 0.62667696 (std:0.111)\n",
      "Parameters: {'weights': 'distance', 'p': 2, 'n_neighbors': 50}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report(knn_random.cv_results_,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=50)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_random.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2: Fitting  on the train data with best parameters and predicing on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.5\n",
      "FB2: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Fit model on train data and predict\n",
    "\n",
    "knn_model=KNeighborsClassifier(n_neighbors=50)\n",
    "knn_model.fit(x_train,y_train)\n",
    "\n",
    "predicted_knn=knn_model.predict(x_test)\n",
    "print('roc_auc_score: {}'.format(roc_auc_score(y_test,predicted_knn)))\n",
    "print('FB2: {}'.format(fbeta_score(y_test,predicted_knn,beta=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "train_score_knn=knn_model.predict_proba(x_train)[:,1]\n",
    "real=y_train\n",
    "print(knn_model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs=np.linspace(0,1,500)\n",
    "\n",
    "beta=2\n",
    "FB_all=[]\n",
    "for cutoff in cutoffs:\n",
    "    predicted=(train_score_knn>cutoff).astype(int)\n",
    "    \n",
    "    TP=((predicted==1) & (real==1)).sum()\n",
    "    TN=((predicted==0) & (real==0)).sum()\n",
    "    FP=((predicted==1) & (real==0)).sum()\n",
    "    FN=((predicted==0) & (real==1)).sum()\n",
    "    \n",
    "    P=TP+FN\n",
    "    N=TN+FP\n",
    "    \n",
    "    precision=TP/(TP+FP)\n",
    "    recall=TP/P\n",
    "    FB=(1+beta**2)*precision*recall/((beta**2)*precision*recall)\n",
    "    FB_all.append(FB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02004008016032064"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mycutoff_knn=cutoffs[FB_all==max(FB_all)][0]\n",
    "mycutoff_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.6101175002599564\n",
      "FB2: 0.3140394088669951\n"
     ]
    }
   ],
   "source": [
    "test_score_knn=knn_model.predict_proba(x_test)[:,1]\n",
    "test_classes_knn=(test_score_knn>mycutoff_knn).astype(int)\n",
    "\n",
    "print('roc_auc_score: {}'.format(roc_auc_score(y_test,test_classes_knn)))\n",
    "print('FB2: {}'.format(fbeta_score(y_test,test_classes_knn,beta=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "naive=GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2: Fitting on the train data with best parameters and predicing on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.5611937194551315\n",
      "FB2: 0.2891692954784437\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model on train and predicting on test\n",
    "naive.fit(x_train,y_train)\n",
    "\n",
    "predict_naive=naive.predict(x_test)\n",
    "\n",
    "print('roc_auc_score: {}'.format(roc_auc_score(y_test,predict_naive)))\n",
    "print('FB2: {}'.format(fbeta_score(y_test,predict_naive,beta=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "train_score_naive=naive.predict_proba(x_train)[:,1]\n",
    "real=y_train\n",
    "print(naive.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs=np.linspace(0,1,500)\n",
    "\n",
    "beta=2\n",
    "FB_all=[]\n",
    "for cutoff in cutoffs:\n",
    "    predicted=(train_score_naive>cutoff).astype(int)\n",
    "    \n",
    "    TP=((predicted==1) & (real==1)).sum()\n",
    "    TN=((predicted==0) & (real==0)).sum()\n",
    "    FP=((predicted==1) & (real==0)).sum()\n",
    "    FN=((predicted==0) & (real==1)).sum()\n",
    "    \n",
    "    P=TP+FN\n",
    "    N=TN+FP\n",
    "    \n",
    "    precision=TP/(TP+FP)\n",
    "    recall=TP/P\n",
    "    FB=(1+beta**2)*precision*recall/((beta**2)*precision*recall)\n",
    "    FB_all.append(FB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03006012024048096"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mycutoff_naive=cutoffs[FB_all==max(FB_all)][0]\n",
    "mycutoff_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.5483102838723095\n",
      "FB2: 0.2829218106995884\n"
     ]
    }
   ],
   "source": [
    "test_score_naive=naive.predict_proba(x_test)[:,1]\n",
    "test_classes_naive=(test_score_naive>mycutoff_naive).astype(int)\n",
    "\n",
    "print('roc_auc_score: {}'.format(roc_auc_score(y_test,test_classes_naive)))\n",
    "print('FB2: {}'.format(fbeta_score(y_test,test_classes_naive,beta=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf=svm.SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1: Hyper parameter tuning using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "       'C':[0.2,0.5,0.8,1],\n",
    "       'kernel' : ['rbf', 'sigmoid'],\n",
    "       'gamma':[0.2,0.5,0.8,1],\n",
    "       'class_weight':['balanced',None]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 416 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:   10.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=SVC(), n_iter=50, n_jobs=-1,\n",
       "                   param_distributions={'C': [0.2, 0.5, 0.8, 1],\n",
       "                                        'class_weight': ['balanced', None],\n",
       "                                        'gamma': [0.2, 0.5, 0.8, 1],\n",
       "                                        'kernel': ['rbf', 'sigmoid']},\n",
       "                   scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_random=RandomizedSearchCV(clf,param_distributions=params,cv=10,scoring='roc_auc',n_iter=50,\n",
    "                             verbose=1,n_jobs=-1)\n",
    "svm_random.fit(x_validation,y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean Validation Score: 0.73652635 (std:0.088)\n",
      "Parameters: {'kernel': 'sigmoid', 'gamma': 0.2, 'class_weight': 'balanced', 'C': 0.2}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean Validation Score: 0.72789064 (std:0.083)\n",
      "Parameters: {'kernel': 'sigmoid', 'gamma': 0.2, 'class_weight': 'balanced', 'C': 0.5}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean Validation Score: 0.71043011 (std:0.074)\n",
      "Parameters: {'kernel': 'sigmoid', 'gamma': 0.2, 'class_weight': 'balanced', 'C': 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report(svm_random.cv_results_,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.2, class_weight='balanced', gamma=0.2, kernel='sigmoid')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_random.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2: Fitting  on the train data with best parameters and predicing on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.6223042528855152\n",
      "FB2: 0.3183229813664596\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model on train and predicting on test\n",
    "svm_model=svm.SVC(C=0.2, class_weight='balanced', gamma=0.2, kernel='sigmoid',probability=True)\n",
    "svm_model.fit(x_train,y_train)\n",
    "\n",
    "predict_svm=svm_model.predict(x_test)\n",
    "\n",
    "print('roc_auc_score: {}'.format(roc_auc_score(y_test,predict_svm)))\n",
    "print('FB2: {}'.format(fbeta_score(y_test,predict_svm,beta=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearSVC_classifier = SklearnClassifier(SVC(kernel='linear',probability=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "train_score_svm=svm_model.predict_proba(x_train)[:,1]\n",
    "real=y_train\n",
    "print(svm_model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs=np.linspace(0,1,500)\n",
    "\n",
    "beta=2\n",
    "FB_all=[]\n",
    "for cutoff in cutoffs:\n",
    "    predicted=(train_score_svm>cutoff).astype(int)\n",
    "    \n",
    "    TP=((predicted==1) & (real==1)).sum()\n",
    "    TN=((predicted==0) & (real==0)).sum()\n",
    "    FP=((predicted==1) & (real==0)).sum()\n",
    "    FN=((predicted==0) & (real==1)).sum()\n",
    "    \n",
    "    P=TP+FN\n",
    "    N=TN+FP\n",
    "    \n",
    "    precision=TP/(TP+FP)\n",
    "    recall=TP/P\n",
    "    FB=(1+beta**2)*precision*recall/((beta**2)*precision*recall)\n",
    "    FB_all.append(FB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004008016032064128"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mycutoff_svm=cutoffs[FB_all==max(FB_all)][0]\n",
    "mycutoff_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_score: 0.5196319018404908\n",
      "FB2: 0.27365491651205937\n"
     ]
    }
   ],
   "source": [
    "test_score_svm=svm_model.predict_proba(x_test)[:,1]\n",
    "test_classes_svm=(test_score_svm>mycutoff_svm).astype(int)\n",
    "\n",
    "print('roc_auc_score: {}'.format(roc_auc_score(y_test,test_classes_svm)))\n",
    "print('FB2: {}'.format(fbeta_score(y_test,test_classes_svm,beta=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
